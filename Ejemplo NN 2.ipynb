{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Estimación de edad de personas\n",
    "El problema de inferir ciertas características de una persona a través de una foto de ella puede resultar bastante dificil incluso para nosotros, como por ejemplo de qué país es, la emoción que expresa, la edad que tiene, o el género. La automatización de este proceso para que máquinas logren identificar ciertas características de una persona puede ser algo crucial para el futuro desarrollo de Inteligencia Artificial.\n",
    "\n",
    "<img src=\"https://i.imgur.com/6B072GE.jpg\" width=\"60%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "Trabajaremos con unos datos (imágenes) en el **objetivo** de predecir la **edad** (*target value*) de la persona presente en la imagen. Los datos corresponden a 3640 imágenes de rostros de personas extraídos de la plataforma Flickr, pero, debido a que trabajamos con redes *feed forward*, se trabajará con representaciones de alto nivel, extraídas manualmente (no-aprendibles). Para ésto necesitará descargar los datos del siguiente __[link](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)__ en el extracto de *ageGenderClassification* o a través de la consola Unix.\n",
    "```\n",
    "wget http://chenlab.ece.cornell.edu/projects/ImagesOfGroups/ageGenderClassification.zip\n",
    "```\n",
    "\n",
    "\n",
    "**Nota:** Enunciado y la parte inicial del código de importe de datos al entorno fue extraido de una tarea para el curso de Machine Learning de la UTFSM - 2019, por lo que los creditos correspondientes de estos elementos corresponden al ayudante y/o profesor del curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "mat_file = sio.loadmat(\"AgeGenderClassification/eventrain.mat\")\n",
    "mat_file_test = sio.loadmat(\"AgeGenderClassification/eventest.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mat_file[\"trcoll\"][0][0] # is \"tecoll\" for testing set\n",
    "age_true = data[1] #target\n",
    "genFeat = data[0]   # Contextual features\n",
    "ffcoefs = data[3]   # Fisherface space\n",
    "faceGist = data[4]  # GIST features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mat_file_test[\"tecoll\"][0][0] # is \"tecoll\" for testing set\n",
    "age_true_test = data[1] #target\n",
    "genFeat_test = data[0]   # Contextual features\n",
    "ffcoefs_test = data[3]   # Fisherface space\n",
    "faceGist_test = data[4]  # GIST features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de prueba\n",
    "X_test = np.concatenate((genFeat_test, ffcoefs_test, faceGist_test), axis = 1)\n",
    "Y_test = age_true_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de entrenamiento\n",
    "X_train = np.concatenate((genFeat, ffcoefs, faceGist), axis = 1)\n",
    "Y_train = age_true[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_2, x_val, Y_train_2, y_val = train_test_split(X_train, Y_train, random_state = 11235813,\n",
    "                                                      test_size = .3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarización de los atributos (whitening)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler().fit(X_train_2)\n",
    "\n",
    "X_train_2_scaled = std_scaler.transform(X_train_2)\n",
    "x_val_scaled = std_scaler.transform(x_val)\n",
    "X_test_scaled = std_scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.countplot(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.countplot(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators = 100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_reg.fit(X_train_2_scaled, Y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_rf = mean_absolute_percentage_error(Y_test, rf_reg.predict(X_test_scaled)).round(4)\n",
    "mse_rf = np.sqrt(mean_squared_error(Y_test, rf_reg.predict(X_test_scaled))).round(4)\n",
    "mae_rf = mean_absolute_error(Y_test, rf_reg.predict(X_test_scaled)).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Feed-forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nn = Sequential()\n",
    "base_nn.add(\n",
    "    Dense(30, input_dim = X_train.shape[1],\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         activation = 'relu',\n",
    "         name = 'input_layer')\n",
    ")\n",
    "\n",
    "base_nn.add(\n",
    "    Dense(50,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         activation = 'relu',\n",
    "          kernel_regularizer = 'l2',\n",
    "         name = 'hidden_1')\n",
    ")\n",
    "\n",
    "base_nn.add(\n",
    "    Dense(1,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         activation = 'tanh',\n",
    "         name = 'out')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "\n",
    "base_nn.compile(optimizer=Adagrad(), loss = 'mean_absolute_error', metrics = ['mse'])\n",
    "base_nn.fit(X_train_2_scaled, Y_train_2, epochs = 100, batch_size = 128, verbose = 0,\n",
    "            callbacks=[history], validation_data=(x_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "metric_train = history.history['mean_squared_error']\n",
    "metric_val = history.history['val_mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "print('Final loss (Validation): {0}'.format(loss_val[-1].round(4)))\n",
    "\n",
    "fig,_ = plt.subplots(figsize = (16,8))\n",
    "plt.title('MAE across training for 200 epoch', size = 20)\n",
    "plt.xlabel('Epoch', size = 15)\n",
    "plt.ylabel('Mean Absolute Error', size = 15)\n",
    "plt.plot(range(1,101), loss_train, '-', label = 'Train MAE');\n",
    "plt.plot(range(1,101), loss_val, '-', label = 'Validation MAE');\n",
    "#plt.plot(range(1,101), metric_val, '-', label = 'Validation Metric: MSE');\n",
    "#plt.plot(range(1,101), metric_train, '-', label = 'Train Metric: MSE');\n",
    "plt.legend();\n",
    "sn.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = std_scaler.transform(X_test)\n",
    "\n",
    "def calculate_metrics(model, x_test, y_test):\n",
    "    mape = mean_absolute_percentage_error(y_test, model.predict(x_test)).round(4)\n",
    "    mse= mean_squared_error(y_test, model.predict(x_test)).round(4)\n",
    "    mae = mean_absolute_error(y_test, model.predict(x_test)).round(4)\n",
    "    \n",
    "    print('\\n##################### {} ##################### '.format(type(model).__name__))\n",
    "    print('MAPE: {}'.format(mape))\n",
    "    print('MSE: {}'.format(mse))\n",
    "    print('MAE: {}'.format(mae))\n",
    "    return (mape,mse,mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = [rf_reg, base_nn]\n",
    "\n",
    "for model in trained_models:\n",
    "    calculate_metrics(model, X_test_scaled, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(base_nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.distplot(base_nn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.distplot(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Otra arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import maxnorm\n",
    "\n",
    "simple_nn = Sequential()\n",
    "\n",
    "simple_nn.add(Dropout(0.2,input_shape = (X_train.shape[1],)))\n",
    "simple_nn.add(\n",
    "    Dense(200,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         activation = 'relu',\n",
    "         name = 'input_layer')\n",
    ")\n",
    "simple_nn.add(Dropout(0.2))\n",
    "\n",
    "simple_nn.add(\n",
    "    Dense(500,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         kernel_constraint=maxnorm(3),\n",
    "         activation = 'tanh',\n",
    "         name = 'hidden_1')\n",
    ")\n",
    "simple_nn.add(Dropout(0.2))\n",
    "\n",
    "simple_nn.add(\n",
    "    Dense(500,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         kernel_constraint=maxnorm(3),\n",
    "         activation = 'relu',\n",
    "         name = 'hidden_2')\n",
    ")\n",
    "simple_nn.add(Dropout(0.2))\n",
    "\n",
    "simple_nn.add(\n",
    "    Dense(500,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         kernel_constraint=maxnorm(3),\n",
    "         activation = 'tanh',\n",
    "         name = 'hidden_3')\n",
    ")\n",
    "simple_nn.add(Dropout(0.2))\n",
    "\n",
    "simple_nn.add(\n",
    "    Dense(500,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         kernel_constraint=maxnorm(3),\n",
    "         activation = 'tanh',\n",
    "         name = 'hidden_4')\n",
    ")\n",
    "simple_nn.add(Dropout(0.2))\n",
    "\n",
    "simple_nn.add(\n",
    "    Dense(500,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         kernel_constraint=maxnorm(3),\n",
    "         activation = 'relu',\n",
    "         name = 'hidden_5')\n",
    ")\n",
    "\n",
    "\n",
    "simple_nn.add(\n",
    "    Dense(200,\n",
    "         kernel_initializer = 'glorot_uniform',\n",
    "         activation = 'relu',\n",
    "         name = 'hidden_6')\n",
    ")\n",
    "simple_nn.add(\n",
    "    Dense(1, activation = 'tanh', name = 'out')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "history_simple_nn = History()\n",
    "\n",
    "#sgd = SGD(lr=0.1, momentum=0.9)\n",
    "\n",
    "simple_nn.compile(optimizer=Adam(), loss = 'mean_absolute_error')\n",
    "\n",
    "simple_nn.fit(X_train_2_scaled, Y_train_2, epochs = 300, batch_size = 16, verbose = 1,\n",
    "            callbacks=[history_simple_nn], validation_data=(x_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_simple = history_simple_nn.history['loss']\n",
    "loss_val_simple = history_simple_nn.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "print('Final loss (Validation): {0}'.format(loss_val[-1].round(4)))\n",
    "\n",
    "fig,_ = plt.subplots(figsize = (16,8))\n",
    "plt.title('MAE across training for 100 epoch', size = 20)\n",
    "plt.xlabel('Epoch', size = 15)\n",
    "plt.ylabel('Mean Absolute Error', size = 15)\n",
    "plt.plot(range(1,101), loss_train_simple, '-', label = 'Train MAE');\n",
    "plt.plot(range(1,101), loss_val_simple, '-', label = 'Validation MAE');\n",
    "plt.legend();\n",
    "sn.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = [rf_reg, base_nn, simple_nn]\n",
    "for model in trained_models:\n",
    "    calculate_metrics(model, X_test_scaled, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.distplot(simple_nn.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(simple_nn.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.countplot(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
